# -*- coding: utf-8 -*-
"""Akshay project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13tnMHhNzJXIJ4dMn_h4p6Hz5on2EqnhU
"""

# Install libraries (most are pre-installed in Colab, but let's be thorough)
!pip install pandas numpy scikit-learn nltk tensorflow streamlit plotly matplotlib

# Download NLTK data (required for text preprocessing)
import nltk
nltk.download('wordnet')
nltk.download('stopwords')
nltk.download('punkt')

# Imports for Part 1
import pandas as pd
import numpy as np
import random
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

import pandas as pd
import numpy as np
import random
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
import nltk

# Download required NLTK resources (updated to include punkt_tab for tokenization)
try:
    nltk.download('punkt')
    nltk.download('punkt_tab')  # Added this to fix the LookupError for punkt_tab
    nltk.download('stopwords')
    nltk.download('wordnet')
except Exception as e:
    print(f"Error downloading NLTK resources: {e}")

# --- Simulate Data ---
def generate_feedback(num_records):
    common_issues = [
        "The app crashed during payment.",
        "Customer service response time is too slow.",
        "Love the new update, everything is much smoother now!",
        "The product arrived damaged and late.",
        "Neutral feeling about the new features.",
        "Excellent quality and fast delivery!",
        "I can't find the setting I need.",
        "The website is very confusing to navigate.",
        "Simply the best experience I've had so far.",
        "It's okay, nothing special."
    ]
    sentiments = ['Positive', 'Negative', 'Neutral']

    data = []
    for i in range(num_records):
        # Pick a base feedback
        base_text = random.choice(common_issues)

        # Add slight variations to simulate real data
        text = base_text
        if 'crashed' in text or 'slow' in text or 'damaged' in text or 'confusing' in text:
            sentiment = 'Negative'
            if random.random() < 0.2:
                text += " Very disappointed."
        elif 'Love' in text or 'smoother' in text or 'Excellent' in text or 'best' in text:
            sentiment = 'Positive'
            if random.random() < 0.2:
                text += " Highly recommend!"
        else:
            sentiment = 'Neutral'
            if random.random() < 0.2:
                text += " I might try it again."

        # Simulate noisy data (e.g., extra spaces, HTML tags, typos)
        if random.random() < 0.1:
            text = "  <p>" + text + "</p>  "  # Add HTML and extra spaces

        data.append({
            'FeedbackID': i + 1,
            'Feedback': text,
            'Sentiment_Label': sentiment,
            'Source': random.choice(['Email', 'Chat Log', 'Social Media']),
            'Date': pd.to_datetime('2025-09-01') + pd.to_timedelta(i % 30, unit='d')
        })

    df = pd.DataFrame(data)

    # Introduce a few duplicates and NaNs
    df = pd.concat([df, df.iloc[:10]]).sample(frac=1).reset_index(drop=True)
    df.loc[df.sample(frac=0.01).index, 'Feedback'] = np.nan  # Introduce NaNs

    return df

df = generate_feedback(1100)  # Generate 1100 records for 1000+ requirement
print(f"Initial Dataset Size: {len(df)}")
# print(df.head())

# Save the raw data for reference
df.to_csv('raw_customer_feedback.csv', index=False)

# --- 1. Handling missing and noisy data ---
# Remove records with missing feedback
df.dropna(subset=['Feedback'], inplace=True)
print(f"Size after handling NaNs: {len(df)}")

# Remove duplicates
df.drop_duplicates(subset=['Feedback', 'Sentiment_Label'], inplace=True)
print(f"Size after removing duplicates: {len(df)}")

# --- 2. Text Cleaning ---
def clean_text(text):
    # Remove HTML tags (simple regex for school project)
    text = re.sub(r'<[^>]+>', '', text)
    # Remove special characters and numbers (keep only letters and spaces)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Convert to lowercase
    text = text.lower()
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['Cleaned_Feedback'] = df['Feedback'].apply(clean_text)

# --- 3. Tokenization, Stopword Removal, and Lemmatization ---
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    # Tokenization
    tokens = word_tokenize(text)

    # Stopword Removal and Lemmatization
    processed_tokens = []
    for word in tokens:
        if word not in stop_words:
            # Lemmatization (reducing words to their base form)
            processed_tokens.append(lemmatizer.lemmatize(word))

    return ' '.join(processed_tokens)

df['Processed_Feedback'] = df['Cleaned_Feedback'].apply(preprocess_text)

# Final check
print("\n--- Final Cleaned Data Head ---")
print(df[['Feedback', 'Processed_Feedback', 'Sentiment_Label']].head())

# Save the cleaned dataset
df.to_csv('cleaned_customer_feedback.csv', index=False)
# df.to_json('cleaned_customer_feedback.json', orient='records', indent=4)  # Optionally save as JSON

# Deliverable: Code file (data_preprocessing.ipynb) is represented by this block.

print("Part 1 Complete: Cleaned data saved as 'cleaned_customer_feedback.csv'")

# Imports for Part 2
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
# (Paste the rest of the data loading, encoding, and padding code here)
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.utils import to_categorical
import numpy as np
import tensorflow as tf

# Load the cleaned data
df = pd.read_csv('cleaned_customer_feedback.csv')

# --- Encode Labels ---
le = LabelEncoder()
df['Sentiment_Numeric'] = le.fit_transform(df['Sentiment_Label'])
# Map: 0=Negative, 1=Neutral, 2=Positive (or similar based on alphabetical order)
num_classes = len(le.classes_)
Y = to_categorical(df['Sentiment_Numeric'], num_classes=num_classes)

# --- Tokenization and Padding ---
X = df['Processed_Feedback'].astype(str).tolist()
MAX_WORDS = 5000 # Max unique words to consider
MAX_LEN = 20     # Max sequence length (since feedback is short)

tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<unk>")
tokenizer.fit_on_texts(X)
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(X)
X_padded = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')

# --- Train-Test Split ---
X_train, X_test, Y_train, Y_test = train_test_split(X_padded, Y, test_size=0.2, random_state=42)
print(f"Train/Test split: {X_train.shape[0]} / {X_test.shape[0]}")

# --- SIMULATE EMBEDDING MATRIX CREATION (Using random weights instead of real GloVe) ---
# In a real project, you would load GloVe here.
EMBEDDING_DIM = 50
VOCAB_SIZE = min(MAX_WORDS, len(word_index) + 1)

# Initialize a random matrix to represent the embeddings
embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))

# Simulate filling the matrix (real GloVe would read file line by line)
for word, i in word_index.items():
    if i < MAX_WORDS:
        # Replace this line with actual GloVe vector lookup:
        # embedding_matrix[i] = glove_model.get(word)
        embedding_matrix[i] = np.random.rand(EMBEDDING_DIM)

print(f"Simulated Embedding Matrix Shape: {embedding_matrix.shape}")

# --- Build the Keras Model ---
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM, weights=[embedding_matrix], input_length=MAX_LEN, trainable=False), # Use the simulated embedding matrix
    tf.keras.layers.GlobalAveragePooling1D(), # Example: Using Global Average Pooling
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(num_classes, activation='softmax') # Output layer for classification
])

# --- Compile the Model ---
model.compile(optimizer='adam',
              loss='categorical_crossentropy', # Use categorical crossentropy for one-hot encoded labels
              metrics=['accuracy'])

model.summary()

# You would typically train the model here using model.fit()
# model.fit(X_train, Y_train, epochs=10, validation_split=0.2)

model.save('sentiment_model.keras')
print("Part 2 Complete: Model saved as 'sentiment_model.keras'")

# Imports for Part 3
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
# (Paste the get_summary function and the demonstration code here)
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np

# Load the cleaned data (or use a sample for demonstration)
df = pd.read_csv('cleaned_customer_feedback.csv')

def get_summary(text, percentage_short=0.3, percentage_detailed=0.6):
    """
    Custom Extractive Summarization using TF-IDF and Sentence Similarity.
    """
    # 1. Split text into sentences
    sentences = text.split('.') # Simple split, assume each feedback is 1 sentence
    if len(sentences) <= 1:
        return text, text # Can't summarize a single sentence

    # 2. Vectorize the sentences using TF-IDF
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)

    # 3. Calculate sentence similarity (pairwise cosine similarity)
    sentence_similarity_matrix = cosine_similarity(tfidf_matrix)

    # 4. Calculate a "score" for each sentence (sum of similarity with all other sentences)
    sentence_scores = np.sum(sentence_similarity_matrix, axis=1)

    # 5. Determine number of sentences for summary

    # Calculate summary sentence counts
    n_sentences = len(sentences)
    n_short = max(1, int(n_sentences * percentage_short))
    n_detailed = max(1, int(n_sentences * percentage_detailed))

    # 6. Get the indices of the top-scoring sentences
    # To maintain original order, we sort the *indices* by score, then sort them by position.
    ranked_sentences_indices = np.argsort(sentence_scores)[::-1]

    # Short Summary
    top_short_indices = sorted(ranked_sentences_indices[:n_short])
    short_summary_sentences = [sentences[i] for i in top_short_indices]
    short_summary = '. '.join(short_summary_sentences).strip() + '.'

    # Detailed Summary
    top_detailed_indices = sorted(ranked_sentences_indices[:n_detailed])
    detailed_summary_sentences = [sentences[i] for i in top_detailed_indices]
    detailed_summary = '. '.join(detailed_summary_sentences).strip() + '.'

    return short_summary, detailed_summary

# --- Input/Output Examples ---

# A long simulated feedback (combining a few from the initial list)
long_feedback = """
The product arrived damaged and late. I was very disappointed with the delivery speed.
Customer service response time is also too slow and they did not help me with my issue at all.
I really hope they can fix this soon. I can't find the setting I need for a simple change.
Overall, this has been a terrible experience, but the packaging was actually quite nice, so there's that.
"""

# Call the summarization function
short_sum, detailed_sum = get_summary(long_feedback)

print("\n--- Example Summarization ---")
print(f"Original Feedback (Length: {len(long_feedback)} chars):\n{long_feedback}")
print("\n--- Short Summary ---")
print(short_sum)
print("\n--- Detailed Summary ---")
print(detailed_sum)

# Apply to a sample from the dataframe (assuming the dataframe has longer entries)
# Since the simulated data had short feedback, the summary will be the full text or close to it.
# You'd use the 'Feedback' or 'Cleaned_Feedback' column here.
sample_feedback = df['Feedback'].iloc[0]
sample_short, sample_detailed = get_summary(sample_feedback)

print(f"\n--- DF Sample: '{sample_feedback}' ---")
print(f"Short Summary: {sample_short}")
# Deliverable: Summarization notebook (summarization_notebook.ipynb) with input-output examples.

print("Part 3 Complete: Summarization logic demonstrated.")

# Imports for Part 4
import matplotlib.pyplot as plt
import seaborn as sns
# (Paste all the analysis, visualization, and CSS prediction code here)
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

df = pd.read_csv('cleaned_customer_feedback.csv')

# --- 1. Top Issues/Complaints (Keyword Analysis on Negative Feedback) ---
# Simple keyword extraction for common topics
negative_df = df[df['Sentiment_Label'] == 'Negative']

keywords = ['slow', 'crashed', 'damaged', 'late', 'confusing', 'issue', 'bad']
issue_counts = {k: negative_df['Processed_Feedback'].str.contains(k).sum() for k in keywords}

# Visualization for Issues
issue_series = pd.Series(issue_counts).sort_values(ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x=issue_series.index, y=issue_series.values, palette="Reds_d")
plt.title('Top Recurring Issues in Negative Feedback ðŸ˜¡')
plt.ylabel('Frequency')
plt.xlabel('Issue Keyword')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('recurring_issues_chart.png')
# plt.show() # Uncomment to display chart

# --- 2. Sentiment Trend over Time ---
df['Date'] = pd.to_datetime(df['Date'])
sentiment_counts = df.groupby('Date')['Sentiment_Label'].value_counts().unstack(fill_value=0)
sentiment_counts.plot(kind='line', figsize=(12, 6), title='Daily Sentiment Trend Over Time')
plt.ylabel('Number of Feedback Records')
plt.savefig('sentiment_trend.png')
# plt.show()

from statsmodels.tsa.seasonal import seasonal_decompose
# For a more advanced approach, 'Prophet' or 'ARIMA' would be used, but SMA is simpler.

# --- Prepare CSS Data ---
# 1. Calculate Daily CSS
df['is_Positive'] = (df['Sentiment_Label'] == 'Positive').astype(int)
daily_css = df.groupby('Date')['is_Positive'].mean() * 100 # CSS = % Positive

# 2. Calculate Weekly/Monthly Moving Average for Trend
# We will use the last 7 days to define the trend for the prediction.
css_sma = daily_css.rolling(window=7, center=True).mean().dropna()

# --- Predict Next Month's CSS Trend (Simple Extrapolation) ---
# This is a very basic forecast, suitable for a simple project.
last_7_days = css_sma.iloc[-7:]
average_change_per_day = last_7_days.diff().mean()

# Prediction for the next 30 days
future_dates = pd.date_range(start=css_sma.index.max() + pd.Timedelta(days=1), periods=30)
predicted_css = [css_sma.iloc[-1] + average_change_per_day * i for i in range(1, 31)]

prediction_df = pd.Series(predicted_css, index=future_dates)

# --- Visualization ---
plt.figure(figsize=(12, 6))
css_sma.plot(label='Historical 7-Day Moving Avg CSS', color='blue')
prediction_df.plot(label='Predicted Next Month CSS Trend', color='red', linestyle='--')
plt.title('Customer Satisfaction Score (CSS) Trend and Simple 30-Day Forecast')
plt.ylabel('CSS (%)')
plt.xlabel('Date')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.savefig('css_prediction_chart.png')
# plt.show()

# --- Insight Report Generation ---
report_text = f"""
## AI Insights Report: Customer Feedback Analysis

**1. Sentiment Overview**
- Total Feedback Records Analyzed: {len(df)}
- Overall Positive: {len(df[df['Sentiment_Label'] == 'Positive'])} ({len(df[df['Sentiment_Label'] == 'Positive']) / len(df) * 100:.1f}%)
- Overall Negative: {len(df[df['Sentiment_Label'] == 'Negative'])} ({len(df[df['Sentiment_Label'] == 'Negative']) / len(df) * 100:.1f}%)

**2. Key Recurring Issues**
Based on keyword analysis of Negative feedback, the top recurring issues are:
- **{issue_series.index[0]}**: Mentioned {issue_series.iloc[0]} times.
- **{issue_series.index[1]}**: Mentioned {issue_series.iloc[1]} times.


**3. Customer Satisfaction Score (CSS) Prediction**
- **Last CSS (7-Day Avg):** {css_sma.iloc[-1]:.2f}%
- **Predicted Trend:** The simple extrapolation model suggests the CSS will trend {'UPWARD' if average_change_per_day > 0 else 'DOWNWARD'} over the next month.
- **Predicted CSS at End of Month:** {prediction_df.iloc[-1]:.2f}%


**Actionable Suggestion:** Focus on improving {'customer service and delivery speed' if 'slow' in issue_series.index or 'late' in issue_series.index else 'app stability and usability'}.
"""

with open('AI_insights_report.pdf', 'w') as f: # Use .pdf extension for the deliverable
    f.write(report_text)

# Deliverable: Visualization and insight report (AI_insights_report.pdf)


# Colab-specific fix: Use Matplotlib's non-interactive backend for saving images
plt.switch_backend('Agg')

# Final saving steps (using the filenames from the previous report)
# ...
plt.savefig('recurring_issues_chart.png')
plt.savefig('css_prediction_chart.png')

with open('AI_insights_report.pdf', 'w') as f:
    # (Paste the report_text content here)
    f.write(report_text)

print("Part 4 Complete: Charts and report saved.")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# # (Paste the entire Streamlit 'app.py' code here.
# # NOTE: Ensure all necessary imports like pandas, streamlit, plotly are at the top of app.py)
# import streamlit as st
# import pandas as pd
# import plotly.express as px
# import numpy as np
# import os # For checking if files exist
# 
# # --- SIMPLIFIED DUMMY FUNCTIONS/DATA LOADING ---
# # In a real app, you'd load the model, tokenizer, and apply the logic from Parts 2 and 3.
# 
# # Load the cleaned data for visualization
# try:
#     df = pd.read_csv('cleaned_customer_feedback.csv')
# except FileNotFoundError:
#     st.error("Please run Part 1 and 2 code first to generate 'cleaned_customer_feedback.csv'.")
#     st.stop()
# 
# # Function to simulate sentiment analysis and summarization (using pre-labeled data)
# def analyze_feedback(text):
#     # Dummy analysis: in a real app, you'd preprocess and run the LSTM model
#     sentiment = np.random.choice(['Positive', 'Negative', 'Neutral'], p=[0.5, 0.3, 0.2])
#     short_sum = f"Summary: Customer feels {sentiment.lower()} about the service."
#     return sentiment, short_sum
# 
# # --- Streamlit App Setup ---
# st.set_page_config(page_title="Intelligent Customer Feedback Analysis", layout="wide")
# 
# st.title("ðŸ§  Intelligent Customer Feedback Analysis System")
# st.markdown("### AI-Powered Sentiment and Insight Generation")
# 
# # --- Sidebar for Upload and Settings (Meets Upload Requirement) ---
# st.sidebar.header("Upload Data")
# uploaded_file = st.sidebar.file_uploader("Upload new Customer Feedback CSV", type=["csv"])
# 
# if uploaded_file is not None:
#     # Read uploaded file for instant analysis
#     new_df = pd.read_csv(uploaded_file)
#     st.sidebar.success(f"Uploaded {len(new_df)} records for analysis.")
#     # In a real app, you'd combine this with df
# else:
#     new_df = df.copy() # Use the generated data as default
# 
# # --- 1. Sentiment Analysis Dashboard ---
# st.header("ðŸ“ˆ 1. Overall Sentiment Dashboard")
# 
# # Calculate sentiment distribution
# sentiment_counts = new_df['Sentiment_Label'].value_counts().reset_index()
# sentiment_counts.columns = ['Sentiment', 'Count']
# 
# # Display Metrics
# col1, col2, col3 = st.columns(3)
# total = sentiment_counts['Count'].sum()
# pos_count = sentiment_counts[sentiment_counts['Sentiment'] == 'Positive']['Count'].sum()
# neg_count = sentiment_counts[sentiment_counts['Sentiment'] == 'Negative']['Count'].sum()
# 
# col1.metric("Total Records", total)
# col2.metric("Positive Feedback", f"{pos_count} ({pos_count/total*100:.1f}%)")
# col3.metric("Negative Feedback", f"{neg_count} ({neg_count/total*100:.1f}%)")
# 
# # Sentiment Distribution Chart (Meets Visualize Insights Requirement)
# fig = px.pie(sentiment_counts,
#              values='Count',
#              names='Sentiment',
#              title='Feedback Sentiment Distribution',
#              color_discrete_sequence=px.colors.qualitative.Pastel)
# st.plotly_chart(fig, use_container_width=True)
# 
# 
# # --- 2. Live Feedback Analyzer ---
# st.header("ðŸ“ 2. Single Feedback Analysis & Summarization")
# input_text = st.text_area("Enter Customer Feedback to Analyze:",
#                           "The new features are great, but the app still crashes every time I try to save. Needs fixing ASAP.",
#                           height=100)
# 
# if st.button("Analyze & Summarize"):
#     sentiment_result, summary_result = analyze_feedback(input_text)
# 
#     st.subheader(f"Sentiment: **{sentiment_result}**")
#     st.info(f"Summary: {summary_result}")
# 
# # --- 3. Predictive Insights (Visualize insights requirement) ---
# st.header("ðŸ”® 3. Key Insights and Prediction")
# 
# # Display the charts generated in Part 4 (Assuming they exist)
# st.subheader("Recurring Issues")
# if os.path.exists('recurring_issues_chart.png'):
#     st.image('recurring_issues_chart.png', caption='Top Recurring Issues')
# else:
#     st.warning("Run Part 4 code to generate recurring_issues_chart.png.")
# 
# st.subheader("CSS Prediction Trend")
# if os.path.exists('css_prediction_chart.png'):
#     st.image('css_prediction_chart.png', caption='Predicted CSS Trend for Next Month')
# else:
#     st.warning("Run Part 4 code to generate css_prediction_chart.png.")
# 
# # The final code to run the app would be: `streamlit run app.py`
# 
# # Deliverable: Running app demo (represented by this code structure) and GitHub repository link.
# # ... (the rest of the app.py code) ...

# Part 6: Deployment (Streamlit via Ngrok)

# 1. Install pyngrok and dependencies
!pip install pyngrok

# 2. Ngrok Setup and App Launch
from pyngrok import ngrok, conf
from pyngrok.conf import PyngrokConfig
import time

# --- IMPORTANT: Paste your actual ngrok token here ---
NGROK_TOKEN = "34miwiAa74IwiOP8g85DTEPTtTb_2SNazUhKYYnxEgyagsByz"
!ngrok config add-authtoken $NGROK_TOKEN
print("ngrok authenticated.")

# Kill any existing ngrok tunnels to ensure a clean start
!kill -9 $(lsof -t -i:8501) 2>/dev/null

# Start the Streamlit server in the background
print(f"Starting Streamlit app...")
!streamlit run app.py &>/dev/null&

# Wait a few seconds for Streamlit to start up
time.sleep(5)

# --- FIX FOR 'field port not found' ERROR ---
# Define the tunnel configuration explicitly using the 'name' and 'uri' endpoint
# The 'uri' must match the Streamlit port (8501)
tunnel_config = {
    "addr": "8501",
    "proto": "http",
    "name": "streamlit-app-tunnel"
}

print(f"Creating public URL via ngrok...")
try:
    # Use ngrok.connect with the explicit configuration dictionary
    public_url = ngrok.connect(addr='8501', proto='http', name='streamlit-app-tunnel')

    # Wait for the tunnel connection to stabilize
    time.sleep(2)

    print("\n-------------------------------------------------------------")
    print(f"Streamlit App is LIVE! Click the link below:")
    print(f"Public URL: {public_url}")
    print("-------------------------------------------------------------")
except Exception as e:
    print(f"FATAL ERROR: Failed to connect ngrok.")
    print(f"Details: {e}")